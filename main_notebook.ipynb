{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\SLHaka\\lib\\site-packages\\nilearn\\datasets\\__init__.py:86: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    },
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x2299ec9d270>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from os.path import exists, join\n",
    "\n",
    "# plotting and visualization\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "np.random.seed(1679838)\n",
    "torch.manual_seed(1679838)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "blessed-register",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-28T09:53:27.478513Z",
     "iopub.status.busy": "2021-06-28T09:53:27.474676Z",
     "iopub.status.idle": "2021-06-28T09:53:35.422989Z",
     "shell.execute_reply": "2021-06-28T09:53:35.421589Z",
     "shell.execute_reply.started": "2021-06-28T06:49:00.608859Z"
    },
    "papermill": {
     "duration": 7.970231,
     "end_time": "2021-06-28T09:53:35.423239",
     "exception": false,
     "start_time": "2021-06-28T09:53:27.453008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = join(\".\", \"data\") if exists(join(\".\", \"data\")) \\\n",
    "    else join(\"..\", \"input\", \"statistical-learning-sapienza-spring-2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train = pd.read_csv(join(data_path, 'train.csv'))\n",
    "test = pd.read_csv(join(data_path, 'test.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Splitting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_split, val_split = train_test_split(train, test_size=0.0333, random_state=42)\n",
    "\n",
    "train_split.reset_index(drop=True, inplace=True)\n",
    "val_split.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_data(entry):\n",
    "    variables = entry[['var1', 'var2', 'var3']].replace({\"A\": 0, \"C\": 1}).to_numpy()\n",
    "    timeseries = entry[5 if \"y\" in entry.index else 4:].to_numpy(dtype=float).reshape((115, 116), order=\"F\")\n",
    "    return {\"variables\": variables, \"timeseries\": timeseries}\n",
    "\n",
    "train_timeseries = train_split.apply(lambda row: get_data(row)[\"timeseries\"], axis=1).to_list()\n",
    "val_timeseries = val_split.apply(lambda row: get_data(row)[\"timeseries\"], axis=1).to_list()\n",
    "train_variables = train_split.apply(lambda row: get_data(row)[\"variables\"], axis=1).to_list()\n",
    "val_variables = val_split.apply(lambda row: get_data(row)[\"variables\"], axis=1).to_list()\n",
    "\n",
    "test_timeseries = test.apply(lambda row: get_data(row)[\"timeseries\"], axis=1).to_list()\n",
    "test_variables = test.apply(lambda row: get_data(row)[\"variables\"], axis=1).to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Correlation matrices and PCA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def apply_connectivity(time_series, vect=True, diag=True):\n",
    "    # objects:\n",
    "    correlation_measure = ConnectivityMeasure(kind=\"tangent\",  # 'partial correlation',\n",
    "                                              vectorize = vect,\n",
    "                                              discard_diagonal=diag)\n",
    "    pca_model = KernelPCA(kernel='rbf', n_components=16)\n",
    "\n",
    "    # fitting predicting:\n",
    "    cor_mat = correlation_measure.fit_transform(time_series)\n",
    "    # pca_model.fit(cor_mat)\n",
    "    reduced_cor = pca_model.fit_transform(cor_mat)\n",
    "    return pd.DataFrame(reduced_cor).add_prefix('cor_')\n",
    "\n",
    "train_cor_vec = apply_connectivity(train_timeseries)\n",
    "val_cor_vec = apply_connectivity(val_timeseries)\n",
    "\n",
    "test_cor_vec = apply_connectivity(test_timeseries)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_var_and_cor = pd.concat([pd.DataFrame(train_variables).add_prefix('var_'), train_cor_vec], axis=1)\n",
    "val_var_and_cor = pd.concat([pd.DataFrame(val_variables).add_prefix('var_'), val_cor_vec], axis=1)\n",
    "\n",
    "test_var_and_cor = pd.concat([pd.DataFrame(test_variables).add_prefix('var_'), test_cor_vec], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data loaders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_dataloader(timeseries, variables_correlation, labels):\n",
    "    return DataLoader(TensorDataset(torch.Tensor(timeseries),torch.Tensor(variables_correlation),\n",
    "                                    torch.Tensor(labels)), batch_size=20) # create your dataloader\n",
    "\n",
    "train_dataloader = get_dataloader(train_timeseries, train_var_and_cor.to_numpy(), train_split.y.to_numpy())\n",
    "val_dataloader = get_dataloader(val_timeseries, val_var_and_cor.to_numpy(), val_split.y.to_numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class BlackEmbedding(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn_model = nn.RNN(input_size=116, hidden_size=hidden_size, num_layers=num_layers, dropout=0.005, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Sequential(nn.Linear(3+16+(hidden_size*(1+num_layers)), 32),\n",
    "                                 nn.ReLU())\n",
    "                                 # nn.Dropout(0.01))\n",
    "        self.fc2 = nn.Sequential(nn.Linear(32, 32),\n",
    "                                 nn.ReLU())\n",
    "                                 # nn.Dropout(0.005))\n",
    "\n",
    "        self.regressor = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x, var_and_cor, predict_anyway=False):\n",
    "        # var_and_cor = var_and_cor[:, :3]\n",
    "\n",
    "        # time series:\n",
    "        outputs, hidden = self.rnn_model(x)\n",
    "        hidden = torch.cat(list(map(lambda i: hidden[i, ], range(hidden.shape[0]))), dim=-1)\n",
    "\n",
    "        x = torch.cat([var_and_cor, outputs[:, -1, ], hidden], dim=-1)  # taking only the last\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        if self.training or predict_anyway:\n",
    "            x = self.regressor(x)\n",
    "\n",
    "        return x.squeeze(dim=1)\n",
    "\n",
    "model = BlackEmbedding(hidden_size=64, num_layers=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 12, 40], gamma=0.9)\n",
    "criterion = nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------> Epoch 1 <------\n",
      "    Train RMSE loss: 109.03999042510986\n",
      "    Val   RMSE loss: 107.3597640991211\n",
      "\n",
      "------> Epoch 2 <------\n",
      "    Train RMSE loss: 101.13809847831726\n",
      "    Val   RMSE loss: 92.09324645996094\n",
      "\n",
      "------> Epoch 3 <------\n",
      "    Train RMSE loss: 74.80802083015442\n",
      "    Val   RMSE loss: 54.28483963012695\n",
      "\n",
      "------> Epoch 4 <------\n",
      "    Train RMSE loss: 34.50653499364853\n",
      "    Val   RMSE loss: 19.818279266357422\n",
      "\n",
      "------> Epoch 5 <------\n",
      "    Train RMSE loss: 18.564527213573456\n",
      "    Val   RMSE loss: 18.640655517578125\n",
      "\n",
      "------> Epoch 6 <------\n",
      "    Train RMSE loss: 18.467590540647507\n",
      "    Val   RMSE loss: 18.16800308227539\n",
      "\n",
      "------> Epoch 7 <------\n",
      "    Train RMSE loss: 17.61947214603424\n",
      "    Val   RMSE loss: 17.666427612304688\n",
      "\n",
      "------> Epoch 8 <------\n",
      "    Train RMSE loss: 17.13859537243843\n",
      "    Val   RMSE loss: 17.30324363708496\n",
      "\n",
      "------> Epoch 9 <------\n",
      "    Train RMSE loss: 17.02143207192421\n",
      "    Val   RMSE loss: 16.90299415588379\n",
      "\n",
      "------> Epoch 10 <------\n",
      "    Train RMSE loss: 16.339134991168976\n",
      "    Val   RMSE loss: 16.588619232177734\n",
      "\n",
      "------> Epoch 11 <------\n",
      "    Train RMSE loss: 16.207583844661713\n",
      "    Val   RMSE loss: 16.377574920654297\n",
      "\n",
      "------> Epoch 12 <------\n",
      "    Train RMSE loss: 16.00371640920639\n",
      "    Val   RMSE loss: 16.044485092163086\n",
      "\n",
      "------> Epoch 13 <------\n",
      "    Train RMSE loss: 15.831758111715317\n",
      "    Val   RMSE loss: 15.730731964111328\n",
      "\n",
      "------> Epoch 14 <------\n",
      "    Train RMSE loss: 15.535844594240189\n",
      "    Val   RMSE loss: 15.406633377075195\n",
      "\n",
      "------> Epoch 15 <------\n",
      "    Train RMSE loss: 15.456043779850006\n",
      "    Val   RMSE loss: 15.119766235351562\n",
      "\n",
      "------> Epoch 16 <------\n",
      "    Train RMSE loss: 15.175158739089966\n",
      "    Val   RMSE loss: 15.816422462463379\n",
      "\n",
      "------> Epoch 17 <------\n",
      "    Train RMSE loss: 15.044970601797104\n",
      "    Val   RMSE loss: 15.462430000305176\n",
      "\n",
      "------> Epoch 18 <------\n",
      "    Train RMSE loss: 14.864282667636871\n",
      "    Val   RMSE loss: 15.137550354003906\n",
      "\n",
      "------> Epoch 19 <------\n",
      "    Train RMSE loss: 14.740972429513931\n",
      "    Val   RMSE loss: 14.855289459228516\n",
      "\n",
      "------> Epoch 20 <------\n",
      "    Train RMSE loss: 14.594873279333115\n",
      "    Val   RMSE loss: 14.611306190490723\n",
      "\n",
      "------> Epoch 21 <------\n",
      "    Train RMSE loss: 14.483179032802582\n",
      "    Val   RMSE loss: 14.409438133239746\n",
      "\n",
      "------> Epoch 22 <------\n",
      "    Train RMSE loss: 14.389677584171295\n",
      "    Val   RMSE loss: 14.220840454101562\n",
      "\n",
      "------> Epoch 23 <------\n",
      "    Train RMSE loss: 14.2987280189991\n",
      "    Val   RMSE loss: 14.090770721435547\n",
      "\n",
      "------> Epoch 24 <------\n",
      "    Train RMSE loss: 14.23117059469223\n",
      "    Val   RMSE loss: 13.987884521484375\n",
      "\n",
      "------> Epoch 25 <------\n",
      "    Train RMSE loss: 14.166325956583023\n",
      "    Val   RMSE loss: 13.907537460327148\n",
      "\n",
      "------> Epoch 26 <------\n",
      "    Train RMSE loss: 14.265511095523834\n",
      "    Val   RMSE loss: 17.596923828125\n",
      "\n",
      "------> Epoch 27 <------\n",
      "    Train RMSE loss: 14.184280127286911\n",
      "    Val   RMSE loss: 16.56517219543457\n",
      "\n",
      "------> Epoch 28 <------\n",
      "    Train RMSE loss: 14.080172210931778\n",
      "    Val   RMSE loss: 15.937153816223145\n",
      "\n",
      "------> Epoch 29 <------\n",
      "    Train RMSE loss: 14.0474351644516\n",
      "    Val   RMSE loss: 15.352872848510742\n",
      "\n",
      "------> Epoch 30 <------\n",
      "    Train RMSE loss: 13.966538518667221\n",
      "    Val   RMSE loss: 15.158773422241211\n",
      "\n",
      "------> Epoch 31 <------\n",
      "    Train RMSE loss: 13.908063977956772\n",
      "    Val   RMSE loss: 15.105281829833984\n",
      "\n",
      "------> Epoch 32 <------\n",
      "    Train RMSE loss: 13.842275649309158\n",
      "    Val   RMSE loss: 15.085468292236328\n",
      "\n",
      "------> Epoch 33 <------\n",
      "    Train RMSE loss: 13.78156691789627\n",
      "    Val   RMSE loss: 15.041604042053223\n",
      "\n",
      "------> Epoch 34 <------\n",
      "    Train RMSE loss: 13.712859481573105\n",
      "    Val   RMSE loss: 15.038025856018066\n",
      "\n",
      "------> Epoch 35 <------\n",
      "    Train RMSE loss: 13.632436007261276\n",
      "    Val   RMSE loss: 14.978689193725586\n",
      "\n",
      "------> Epoch 36 <------\n",
      "    Train RMSE loss: 13.558810085058212\n",
      "    Val   RMSE loss: 14.920589447021484\n",
      "\n",
      "------> Epoch 37 <------\n",
      "    Train RMSE loss: 13.49323433637619\n",
      "    Val   RMSE loss: 14.909741401672363\n",
      "\n",
      "------> Epoch 38 <------\n",
      "    Train RMSE loss: 13.408486127853394\n",
      "    Val   RMSE loss: 14.910157203674316\n",
      "\n",
      "------> Epoch 39 <------\n",
      "    Train RMSE loss: 13.328976690769196\n",
      "    Val   RMSE loss: 14.860590934753418\n",
      "\n",
      "------> Epoch 40 <------\n",
      "    Train RMSE loss: 13.281163454055786\n",
      "    Val   RMSE loss: 14.556272506713867\n",
      "\n",
      "------> Epoch 41 <------\n",
      "    Train RMSE loss: 13.165863126516342\n",
      "    Val   RMSE loss: 14.388833999633789\n",
      "\n",
      "------> Epoch 42 <------\n",
      "    Train RMSE loss: 13.04481366276741\n",
      "    Val   RMSE loss: 14.197061538696289\n",
      "\n",
      "------> Epoch 43 <------\n",
      "    Train RMSE loss: 12.907097071409225\n",
      "    Val   RMSE loss: 14.2140474319458\n",
      "\n",
      "------> Epoch 44 <------\n",
      "    Train RMSE loss: 12.818435341119766\n",
      "    Val   RMSE loss: 14.133919715881348\n",
      "\n",
      "------> Epoch 45 <------\n",
      "    Train RMSE loss: 12.674973726272583\n",
      "    Val   RMSE loss: 14.099133491516113\n",
      "\n",
      "------> Epoch 46 <------\n",
      "    Train RMSE loss: 12.580528110265732\n",
      "    Val   RMSE loss: 14.278770446777344\n",
      "\n",
      "------> Epoch 47 <------\n",
      "    Train RMSE loss: 12.43376049399376\n",
      "    Val   RMSE loss: 14.079383850097656\n",
      "\n",
      "------> Epoch 48 <------\n",
      "    Train RMSE loss: 12.28412476181984\n",
      "    Val   RMSE loss: 13.585318565368652\n",
      "\n",
      "------> Epoch 49 <------\n",
      "    Train RMSE loss: 12.106792539358139\n",
      "    Val   RMSE loss: 13.629054069519043\n",
      "\n",
      "------> Epoch 50 <------\n",
      "    Train RMSE loss: 11.909566849470139\n",
      "    Val   RMSE loss: 13.635793685913086\n",
      "\n",
      "------> Epoch 51 <------\n",
      "    Train RMSE loss: 11.729222804307938\n",
      "    Val   RMSE loss: 13.7078275680542\n",
      "\n",
      "\n",
      "****************** Finished Training ******************\n",
      "seconds elapsed:  196.07907819747925\n"
     ]
    }
   ],
   "source": [
    "epocs = 70\n",
    "validate = True\n",
    "rmse = {\"train\": [], \"val\": []}\n",
    "best = {\"loss\": None, \"state_dict\": None}\n",
    "\n",
    "start = time()\n",
    "for epoch in range(epocs):\n",
    "    model.train()\n",
    "    rmse[\"train\"].append(0)\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, var_and_cor, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, var_and_cor)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        rmse[\"train\"][epoch] += (loss.sqrt().mean(dim=0)/len(train_dataloader)).item()\n",
    "\n",
    "    print(f\"\\n------> Epoch {epoch+1} <------\\n    Train RMSE loss: {rmse['train'][epoch]}\")\n",
    "    # validation\n",
    "    if validate:\n",
    "        model.eval()\n",
    "\n",
    "        rmse[\"val\"].append(0)\n",
    "        for i, data in enumerate(val_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, var_and_cor, labels = data\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs, var_and_cor, predict_anyway=True)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            rmse[\"val\"][epoch] += (criterion(outputs, labels).sqrt().mean(dim=0)/len(val_dataloader)).item()\n",
    "\n",
    "        print(f\"    Val   RMSE loss: {rmse['val'][epoch]}\")\n",
    "\n",
    "        # check for overfitting:\n",
    "        if len(rmse[\"val\"]) > 4 and all(map(lambda x: x[0]<x[1], zip(rmse[\"val\"][-4:-1], rmse[\"val\"][-3:]))):\n",
    "            break\n",
    "\n",
    "        if best[\"loss\"] is None or best[\"loss\"] > rmse[\"val\"][epoch]:\n",
    "            best[\"loss\"] = rmse[\"val\"][epoch]\n",
    "            best[\"state_dict\"] = model.state_dict()\n",
    "\n",
    "    # scheduler.step()\n",
    "\n",
    "print('\\n\\n****************** Finished Training ******************')\n",
    "print(\"seconds elapsed: \", time()-start)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAjs0lEQVR4nO3deXCc9Z3n8fdXh3XbumVZki0DTgAbW5IFeIeEQEgykAPMBIg2ocZQSdgl2U3ITjLjUJNislWZJBWGZVObpJbMcKRCYDxOOCaVi3jMElLhkI1xbIwxh7F1WLdlWbfUv/3j6ZZabdkYdbdaep7Pq+qp5+h+nv49Lfvz/Pr3PM/vMeccIiLiL2mpLoCIiCSewl1ExIcU7iIiPqRwFxHxIYW7iIgPKdxFRHxI4S4i4kMKd/E9MztsZsNmdtLMjpnZg2aWH37tQTNzZnZtzDr3hpffEp5fYmb/ZGYt4e28ZWb/6zSfERn+z7zuqEgUhbsExSecc/lAHVAPfD3qtdeALZEZM8sAbgTeiHrP14FG4BKgALgSeGm2z4ga/lvC90LkLGWkugAi88k5d8zMfosX8hH/DtxsZkXOuT7gamAvXohHXAw85pxrC88fDg8iC5Jq7hIoZlYNXAO8HrV4BHgSaArP/zXwk5hVnwP+h5l9wcwuMjNLemFF4qBwl6B43MwGgKNAJ3BXzOs/Af7azJYBHwAej3n928B3gc8AzUCrmW2Jec/jZnY8avh8ondC5Gwp3CUoNjvnCoArgPOB0ugXnXPPAmXA3wO/dM4Nx7w+6Zz7gXPuMqAQ+BZwv5ldEPMZhVHDj5O3OyJnpnCXQHHO/T/gQeDuWV7+KfA3nNokE7uNYefcD4A+4MJEl1EkEXRCVYLoXuCwmdXFLP8+8AfgmdgVzOwOYA/wPDCO1zxTwKlXzIgsCAp3CRznXJeZ/QT4BjAQtbwX2HGa1YaBfwLOAxze5ZOfdM69GfWefzezyaj5p5xz1ye08CJnyfSwDhER/1Gbu4iIDyncRUR8SOEuIuJDCncRER9aEFfLlJaWutra2lQXQ0RkUdm1a1e3c65sttcWRLjX1tbS3Nyc6mKIiCwqZvb26V5Ts4yIiA8p3EVEfEjhLiLiQwuizV1E/GV8fJyWlhZGRkZSXRRfyM7Oprq6mszMzLNeR+EuIgnX0tJCQUEBtbW16Lkm8XHO0dPTQ0tLC6tXrz7r9dQsIyIJNzIyQklJiYI9AcyMkpKSd/0rSOEuIkmhYE+cuXyXizrcjx6Fr30NOjtTXRIRkYVlUYf7iRNw993w8MOpLomILCTHjx/nhz/84bte76Mf/SjHjx9PfIFSYFGH+9q1cPHF8MADoG7pRSTidOE+OTk5y7un/epXv6KwsDBJpZpfizrcAb54fRuH/jzM7t2pLomILBRbt27ljTfeoK6ujosvvpgrr7yST3/601x00UUAbN68mY0bN7J27Vruu+++qfVqa2vp7u7m8OHDXHDBBXz+859n7dq1fOQjH2F4ePh0H7cgLe5LIZ95hi13foB/y/wtDzzwETZuTHWBRCTWHXfAnj2J3WZdHdx77+lf/853vsO+ffvYs2cPTz/9NB/72MfYt2/f1KWE999/P8XFxQwPD3PxxRfzyU9+kpKSkhnbOHToEI888gg//vGPuemmm/j5z3/OzTffnNgdSaLFXXNfvx6Az1y4m5/9DHS/hIjM5pJLLplxjfj3v/99NmzYwKZNmzh69CiHDh06ZZ3Vq1dTV1cHwMaNGzl8+PA8lTYxFnfNvbAQzj2XDy7dRV8fPPkk3HRTqgslItHOVMOeL3l5eVPTTz/9NL///e/505/+RG5uLldcccWs15BnZWVNTaenpy+6ZpnFXXMHaGigvHU3NTVw//2pLoyILAQFBQUMDAzM+lp/fz9FRUXk5uby6quv8txzz81z6eaHL8Ld3nyT/3JTH7/7HbS0pLpAIpJqJSUlXHbZZaxbt46vfe1rM167+uqrmZiYYP369XzjG99g06ZNKSplci3uZhmAhgYAbqnbw9+7K/nJT+DOO1NcJhFJuZ/97GezLs/KyuLXv/71rK9F2tVLS0vZt2/f1PKvfvWrCS9fsvmi5g5QdWwXl1+ua95FRMAP4V5aCitXwu7d3HorvP46/PGPqS6UiEhqLf5wB6/2vns3N9wAeXle7V1EJMj8E+6vvUa+G+Cmm2DbNhgcTHWhRERSxx/hvnGj19C+Zw+33gonT8L27akulIhI6vgj3MMnVdm9m/e9D847T9e8i0iwvWO4m9n9ZtZpZvuilhWb2VNmdig8Lop67etm9rqZHTSzv0xWwWdYvhwqK2H3bszg1lvhmWfgjTfm5dNFZJHLz88HoK2tjRtuuGHW91xxxRU0NzefcTv33nsvQ0NDU/Op7EL4bGruDwJXxyzbCuxwzq0BdoTnMbMLgSZgbXidH5pZesJKeyYNDbBrFwBNTd6ip56al08WEZ9YsWIF2+No040N91R2IfyO4e6cewbojVl8HfBQePohYHPU8kedc6POubeA14FLElPUd7BxIxw4AENDrFoFGRnek5pEJHj+7u/+bkZ/7v/wD//AN7/5Ta666ioaGhq46KKLeOKJJ05Z7/Dhw6xbtw6A4eFhmpqaWL9+PZ/61Kdm9C1z++2309jYyNq1a7nrrrsArzOytrY2rrzySq688kpgugthgHvuuYd169axbt067g13uJPMroXneodqhXOuHcA5125m5eHlVUB0Rw0t4WXJ19AAoRDs3Uv6pk2sWAFHjszLJ4vImaSgz9+mpibuuOMOvvCFLwCwbds2fvOb3/CVr3yFpUuX0t3dzaZNm7j22mtP+3zSH/3oR+Tm5rJ371727t1LQ+TcHvCtb32L4uJiJicnueqqq9i7dy9f+tKXuOeee9i5cyelpaUztrVr1y4eeOABnn/+eZxzXHrppXzgAx+gqKgoaV0LJ/qE6mzf0qz3i5rZbWbWbGbNXV1d8X9y1ElVgJoa1dxFgqq+vp7Ozk7a2tp4+eWXKSoqorKykjvvvJP169fzoQ99iNbWVjo6Ok67jWeeeWYqZNevX8/6cBfj4B0sGhoaqK+vZ//+/bzyyitnLM+zzz7L9ddfT15eHvn5+fzVX/0Vf/jDH4DkdS0815p7h5lVhmvtlUDkEdUtQE3U+6qBttk24Jy7D7gPoLGxMf4OA6qrvbtVw+3uNTXwwgtxb1VE4pWiPn9vuOEGtm/fzrFjx2hqauLhhx+mq6uLXbt2kZmZSW1t7axd/UabrVb/1ltvcffdd/Piiy9SVFTELbfc8o7bcWfoEyVZXQvPteb+JLAlPL0FeCJqeZOZZZnZamANMD8Ra+a1u4dr7itXej1EhkLz8ukissA0NTXx6KOPsn37dm644Qb6+/spLy8nMzOTnTt38vbbb59x/csvv5yHH34YgH379rF3714ATpw4QV5eHsuWLaOjo2NGJ2Sn62r48ssv5/HHH2doaIjBwUEee+wx3v/+9ydwb0/1jjV3M3sEuAIoNbMW4C7gO8A2M/sscAS4EcA5t9/MtgGvABPAF51zZ34ibSI1NMD3vgejo9TUZDE2Bl1dUFExbyUQkQVi7dq1DAwMUFVVRWVlJZ/5zGf4xCc+QWNjI3V1dZx//vlnXP/222/n1ltvZf369dTV1XHJJd61IRs2bKC+vp61a9dyzjnncNlll02tc9ttt3HNNddQWVnJzp07p5Y3NDRwyy23TG3jc5/7HPX19Ul9upOd6efCfGlsbHTvdP3oWdm+HW68EZqbeaJlI5s3w4svQmNj/JsWkbN34MABLrjgglQXw1dm+07NbJdzbtaE88cdqhGRk6q7dlETbvnXSVURCSJ/hfvq1d5zVXfvngp3XQ4pIkHkr3A3m+r+t7QUsrNVcxdJlYXQ5OsXc/ku/RXu4IX73r3YxLiudRdJkezsbHp6ehTwCeCco6enh+zs7He13uJ/hmqshgYYHYVXXqGmZoPCXSQFqquraWlpISE3KArZ2dlUV1e/q3X8F+4bN3rj3bupqdnAjh2pLY5IEGVmZrJ69epUFyPQ/Ncsc955kJ8/dVK1rQ0mJlJdKBGR+eW/cE9Lg/r6qcshQyFob091oURE5pf/wh28dvc9e6hZ4d0cq8shRSRo/BnuGzfC8DDnTR4EdMWMiASPP8O9vh6Aqu49gMJdRILHn+Eevj0193g7S5cq3EUkePwZ7kuXQmYmdHfrRiYRCSR/hruZ9+AOhbuIBJQ/wx28cO/qUriLSCD5O9zDNffOTniHp2CJiPiKf8O9rGwq3MF75J6ISFD4N9zDNfeVK71ZNc2ISJD4O9x7e6fuUlW4i0iQ+DvcnaM6txdQuItIsPg33MvKAMgZ7Ka0VOEuIsHi33AvLfXGuhxSRALI/+EevmJGPUOKSJD4N9zDzTKRK2ZUcxeRIPFvuJeUeONwzb2/HwYGUlskEZH54t9wz872HrcXbnMH1d5FJDj8G+4wowsCULiLSHD4O9xjuiBQuItIUPg73MM9Q65Y4fUCrHAXkaDwf7h3d5OZCZWVuhxSRILD3+EebpYBdDmkiASKv8O9tBQGB2F4WHepikigxBXuZvYVM9tvZvvM7BEzyzazYjN7yswOhcdFiSrsuxZzl+rRo+BcykojIjJv5hzuZlYFfAlodM6tA9KBJmArsMM5twbYEZ5PjZhwHx6G3t6UlUZEZN7E2yyTAeSYWQaQC7QB1wEPhV9/CNgc52fMXVQXBLocUkSCZM7h7pxrBe4GjgDtQL9z7ndAhXOuPfyedqB8tvXN7DYzazaz5q6urrkW48xieoYEhbuIBEM8zTJFeLX01cAKIM/Mbj7b9Z1z9znnGp1zjWWRGnaixTTLgC6HFJFgiKdZ5kPAW865LufcOPAL4C+ADjOrBAiPO+Mv5hwVFUFaGnR3U1EBmZmquYtIMMQT7keATWaWa2YGXAUcAJ4EtoTfswV4Ir4ixiE9HYqLobubtDSorla4i0gwZMx1Refc82a2HdgNTAAvAfcB+cA2M/ss3gHgxkQUdM7CXRAAutZdRAJjzuEO4Jy7C7grZvEoXi1+YQh3QQBeuP/xjykuj4jIPPD3HaowowuCmhpobYVQKMVlEhFJMv+He0yzzPg4dHSkuEwiIkkWjHDv7gbnWLnSW6TLIUXE7/wf7mVlMDkJ/f26kUlEAsP/4T7LjUwKdxHxu+CEe1cXRUWQkTHVBC8i4lvBCffubsy8e5p6elJbJBGRZPN/uEf1DAlQUqJwFxH/83+4RzXLgMJdRILB/+GelwdZWaq5i0ig+D/czWbcpVpaqnAXEf/zf7jDjP5lIjV3PUtVRPwsOOEe1eY+OgpDQykuk4hIEgUn3KNq7qCmGRHxt2CEe1Sbu8JdRIIgGOFeWgrHj8P4uMJdRAIhOOEO0NOjcBeRQAhGuEfdpapwF5EgCEa4R/UvU1zsTSrcRcTPghXuXV1kZsLSpVPnV0VEfClY4a4uCEQkIBTuIiI+FIxwz8yEZcvUM6SIBEYwwh1m7V9GRMSvFO4iIj4UnHCP6YKgvx8mJlJcJhGRJAlOuMf0DAnQ25vC8oiIJFGwwr27G5zTXaoi4nvBCfeyMhgZgaEhhbuI+F5wwj3qLlWFu4j4XfDCXZ2HiUgAKNxFRHwornA3s0Iz225mr5rZATP7T2ZWbGZPmdmh8LgoUYWNS1S3v/n53k2rCncR8at4a+7/G/iNc+58YANwANgK7HDOrQF2hOdTL6rN3Uw3MomIv8053M1sKXA58C8Azrkx59xx4DrgofDbHgI2x1fEBFm2DNLTdZeqiARCPDX3c4Au4AEze8nM/tnM8oAK51w7QHhcPtvKZnabmTWbWXNX+OaipEpLUxcEIhIY8YR7BtAA/Mg5Vw8M8i6aYJxz9znnGp1zjWWR9vBki7lLVeEuIn4VT7i3AC3OuefD89vxwr7DzCoBwuPO+IqYQKq5i0hAzDncnXPHgKNm9t7woquAV4AngS3hZVuAJ+IqYSLNEu7OpbhMIiJJkBHn+v8deNjMlgBvArfiHTC2mdlngSPAjXF+RuJE9QxZWgrj43DyJBQUpLhcIiIJFle4O+f2AI2zvHRVPNtNmtJSr7oeClFS4v1o6elRuIuI/wTnDlXwwj0Ugr4+3aUqIr4WrHCPuktV4S4ifhascFfPkCISEMEMd9XcRcTnAhvuReHuzBTuIuJHwQr3SJt7VxcZGVBYOHVlpIiIrwQr3HNyID8fOr2bZnWXqoj4VbDCHaC8XOEuIr6ncFe4i4gPBS/cKyqgowNQuIuIfwUv3FVzF5EACGa4d3WF+5eBgQEYG0t1oUREEit44V5R4fUv09s7dSNTb29qiyQikmjBC/fy8FP/Ojp0l6qI+FZww72zU+EuIr6lcEfhLiL+E7xwr6jwxgp3EfGx4IV7cTGkpanNXUR8LXjhnpbmdSDW2UluLmRlKdxFxH+CF+4wdSOTmW5kEhF/Cma4qwsCEfG5YIa7uiAQEZ9TuCvcRcSHghvuJ0/C0JDCXUR8KZjhHnOte28vOJfaIomIJFIwwz3mLtWJCThxIrVFEhFJJIW7bmQSER8KZrhHNcuUlnqTCncR8ZNghntZmTdWFwQi4lPBDPfcXMjPV7OMiPhWMMMdpq51V7iLiB/FHe5mlm5mL5nZL8PzxWb2lJkdCo+L4i9mElRUQGcnhYVgpnAXEX9JRM39y8CBqPmtwA7n3BpgR3h+4Skvh44O0tOhqAi6u1NdIBGRxIkr3M2sGvgY8M9Ri68DHgpPPwRsjuczkkZdEIiIj8Vbc78X+FsgFLWswjnXDhAel8+2opndZmbNZtbc1dUVZzHmoLwcurogFFK4i4jvzDnczezjQKdzbtdc1nfO3eeca3TONZZFLk2cTxUVEApBb6/CXUR8J56a+2XAtWZ2GHgU+KCZ/RToMLNKgPC4M+5SJkPkLtXwte4KdxHxkzmHu3Pu6865audcLdAE/Idz7mbgSWBL+G1bgCfiLmUyxHRBoHAXET9JxnXu3wE+bGaHgA+H5xeemHAfHITR0dQWSUQkUTISsRHn3NPA0+HpHuCqRGw3qWK6/QWv9r5iReqKJCKSKMG9Q7W4GNLS1L+MiPhScMM9Lc3rQExdEIiIDwU33EH9y4iIbwU73MP9yyjcRcRvgh3u4f5lFO4i4jcK985OcnK8Lt47F+btViIi71qww72iAk6ehKEhzj8f9u9PdYFERBIj2OEeuZGpq4u6OtizB5xLZYFERBJD4Q7Q0UFdnddJZHt7SkskIpIQCneAzk7q6rzJl15KWWlERBIm2OEe1QXBhg3e5J49KSuNiEjCBDvcI/3Id3SwdCmce67CXUT8IdjhnpsL+flT10BGTqqKiCx2wQ53mPEs1bo6eP11GBhIbZFEROKlcA93QQBMnVTduzd1xRERSQSFe7gLApgOdzXNiMhip3CPapapqoKSEl0OKSKLn8K9vNy7eykUwkwnVUXEHxTuFRUQCkFvL+CF+759MD6e2mKJiMRD4R7VBQFAfb33oOyDB1NYJhGROCnco7ogAJ1UFRF/ULhHdUEA8N73QlaWwl1EFjeFe0zNPSMDLrpI4S4ii5vCvbgY0tKm2twB9e0uIouewj0tzetALOoZe3V13vNUW1pSVywRkXgo3GFGFwSgk6oisvgp3GHGXaoA69eDmcJdRBYvhTvM6F8GoKAAzjtP4S4ii5fCHU6puYO6IRCRxU3hDl6b+8mTMDQ0taiuDt58E/r7U1csEZG5UrjD9LXuXV1Ti9S3u4gsZgp3OKV/GdAVMyKyuM053M2sxsx2mtkBM9tvZl8OLy82s6fM7FB4XJS44iZJzF2qAJWV3uXv6ttdRBajeGruE8DfOOcuADYBXzSzC4GtwA7n3BpgR3h+YYvpXwa8SyHr61VzF5HFac7h7pxrd87tDk8PAAeAKuA64KHw2x4CNsdZxuQrK/PGx47NWFxXB/v3w9jY/BdJRCQeCWlzN7NaoB54HqhwzrWDdwAAyk+zzm1m1mxmzV1RJzJTIjcXSkvhm9+Epib4/e8hFKKuzgv2V19NbfFERN6tuMPdzPKBnwN3OOdOnO16zrn7nHONzrnGskjNOZWefRZuvx1+9zv48IdhzRquev4fqaRNTTMisuiYi6PrQzPLBH4J/NY5d0942UHgCudcu5lVAk875957pu00Nja65ubmOZcjoUZG4LHH4Mc/hp07mSCdA9n19JZfQEfxBXSXnk9vxQWcrDiX1e/J5OqrYdWqVBdaRILIzHY55xpnfW2u4W5mhtem3uucuyNq+feAHufcd8xsK1DsnPvbM21rQYV7tEOH2LnlQXL2vcCq4VepnJjuJnKcDF7nPPazlo6SteRfupY1111IQ9N7yF66JIWFFpGgSFa4vw/4A/BnIBRefCdeu/s2YCVwBLjROdd7pm0t2HCPNTDgPVz1wAHcKwc4+eIBJvbuZ2nXG6SHv4JxMjiWs5qTJasI1awi+72rKK5bReGGVdjqWqiuhvT01O6HnJ2hIa+prrLSOxezYYN3GZXIApGUcE+kRRPupzM8zPCegxx87BU6du4n/c1DFB5/m+rQ2yynY8Zbxy2TjuxVdOafS8+yc+grOoexFbVUX1rFhR+uonxDJWRmpmhHZIpz8OlPw7/+q3cwnpjwnsHY1OQN55+f6hKKKNxTwTlob4eDL4/Q+qcj9O15m7S336L4xFuUD7xJ5dAbrBh9k8JQ34z1QhgnsssZL6sic2UlVlpCelkx6RUlLKkoJr2sGEpKoKoKamq8Liwl8b77Xdi6Fb79bfjc5+AXv4BHH4Wnn/b+uBs2wNVXw8UXwyWXeL/IVKuXeaZwX8j6+hg79DZvPNPK0edbOb6vlfHDrRSNtFFJO8X0UkwvBZycdfXBjKX05tZwYlk1Q8U1jJZWMbm8ClasIK2miszaKvJrS6lYbpSUeA+emrPecOtaUZG/g+xXv4KPfxxuugkeeWTmvra1wb/9G2zbBi++COPj3vLly6eDvrERGhqm73wWSRKF+yLjHLz1Fuzb53VWefIkDPePEerpw/X0Yj1dLOlsJaf7KPn9LRSebKFs5CjLJ1qooIM0Zv5NR1lCGytopZrenCpOLK1mpLSaUGUVoyUrGC1azmhxJekFuWRlQVYWlBQ7Vk8coubIs5Qc+COZLzyLvfaat8H8fFi5cnpYtcr7FVFd7f2iqK727h1YjA4e9AL63HO9y2PPtB8jI/Dyy17Iv/CCNxw8OP16VZUX8g0N3u3O553nHRgLCyEnx98HSJkXCveAcA6GT4wzcOgYw6+3MvZWK6GjrdDaCm2tLOloIfd4K0WDLWSFRk5Z/wQFtFNJF2W8h9cox7u5rIdinkv7C/YtvYyM3CXUuCOsmDxC5djblA0fYelo9ynbGssvYry8msnlK6BiObZiOenVlSypWU5G9XKvy4eyMi/s4vo5kUDHj8Oll0JfHzQ3eweuuWxjzx7YvXt6ePXVU5+2vmTJdNDn53vnWTIyTh2npXkHgehxWpr3WmRIT58eRx8wItNm3utLlnjbPZvBbPbhdNLSpsscGTIzvX2srFy8B/sFTuEuMznnBVhLi3dioL2dUNsxQq3thNqPwbEOhsprOXbuZby+/H28Zu/lWGca7e1e//aDgzOHyYEhlp5ooXSslWpaqKaFKlqp4SiVtLOcYyznGEsYP6Uok6TRn1HCiawyTmaXMpRbykhOsXdwyC9isqCIUGExrrAIt6wQK/KGtOJCsvIzyc5masjKmjm9ZIk3nNXFSZOTcO213k1sO3bA5Zcn7vseHPRq+EePet/78eMzx4OD3gnb8XFviExPTHh/q1Bo5nhy0hsmJqaHyHxE9P/rUMh7bWzMm06FwkJYsWJ6KC2FZctg6VJviEwvWzZ90CssnL+LC5xblL+kFO4yL0ZHvfDv7/dy6/hxr0lpcBCGBh0TXX2kdx0jvesYmT3HyOzvJutEFzknu8gd6qJgpJul490UTPRRGOolm9Ezft5J8uhnGYPkMUgeJ8mfmh4kj2FyGCaHUbIZy8hhPD2HiYxsJtKzCGVmMZnhjUOZWbz/5K+5qesH3FX+Ix7I+q+MjDA1pKUx1VwVPWRne60rkSEyHzmgpKV548iQmem9Frud2Ap19HxkOnY823sjFe4zCoWmDyJnGpybfZjtAyIHnsiBJvoA1dvrnaeIHlpbveWDg+/8jyovb7oZK1KG6IOd2exfaFbWzC8/8scAOHHC+8cZ/Y91eNj7BVVUNHMoKYE1a+DCC71h9eqF80sThbssVsPDhHr6GGnvY6yjj8me44x3HyfU3YfrOz59BBkaJG1oEBseJG14kPSRQTJGB0kfHyFjfNgbJt+597ff1t7Gg5v+71RIR34BhELegSt2GBnxMiF2GB/31olUsCPD+HjyO6GLtL7MNsx2gIosj31f7AElMh17cJttm9HbXrJkZktNpPUI8A4AAwNe2J44MTNsY3/hDA+f2jxl5n3RY2On/nHGxrwvPfYPAdO/EAoLvfGyZV6z0cDAzM/t6/Me4BPdoWBOjncZ7PnnezsZ/QsqMh19MITp6civp+hhdBSuuQbuvXdOf+8zhXvGnLYoMh9yckirziG3egVxt9hOTk6ncXQARKbT0vjLhgb+MsmVMue8kI/NoTNVoqOzIDIf/drppqPzIzb/Bge9yvPp3hfZRjLqfpHKdm5uBjk5ReTmFpGby4whLy9quswb5+d7Q17e9HRu7swmudhfVAlpaenvhwMHvC5iX3nFG557zgvr2c57RGr20ecpzKaPlPn5M4+m556bgEKeSuEuwZCe7qVCXl5Ki2E2/X96MdyiMDk586AS+8tltl800UP0aYHoVpuxMe8G4NmGSItNZH5w0NvWu2U2fRAoKJg+MER+lUUPOTmnvnd6vIyCgk3kX7WJgs3e8iWLoIcRhbuInFZ6+nQtOJUmJ72Qj1waHBkiwR99sIn8QIu8Z2Bg5jr9/d4TNaPPq0Tef7bnm5csmW7RiQyRVp7Cwukm+9jpyOu5uck/f6twF5EFLz19+sKaZHFuOuQjB4TocewQe1724MHp00DvdK44PX066K+/Hu6+O/H7o3AXEcGrSUfa+eO9uXh8fOZ52b6+6XPFsePq6vjLPhuFu4hIgmVmevfopfI5RAvngk0REUkYhbuIiA8p3EVEfEjhLiLiQwp3EREfUriLiPiQwl1ExIcU7iIiPrQguvw1sy7g7Tg2UQqc+jgg/wra/oL2OSi0z+/OKufcrLdKLYhwj5eZNZ+uT2M/Ctr+gvY5KLTPiaNmGRERH1K4i4j4kF/C/b5UF2CeBW1/QfscFNrnBPFFm7uIiMzkl5q7iIhEUbiLiPjQog53M7vazA6a2etmtjXV5UkGM7vfzDrNbF/UsmIze8rMDoXHRaksY6KZWY2Z7TSzA2a238y+HF7u2/02s2wze8HMXg7v8zfDy327zwBmlm5mL5nZL8Pzft/fw2b2ZzPbY2bN4WVJ2edFG+5mlg78ALgGuBD4z2Z2YWpLlRQPAlfHLNsK7HDOrQF2hOf9ZAL4G+fcBcAm4Ivhv62f93sU+KBzbgNQB1xtZpvw9z4DfBk4EDXv9/0FuNI5Vxd1bXtS9nnRhjtwCfC6c+5N59wY8ChwXYrLlHDOuWeA3pjF1wEPhacfAjbPZ5mSzTnX7pzbHZ4ewPvPX4WP99t5ToZnM8ODw8f7bGbVwMeAf45a7Nv9PYOk7PNiDvcq4GjUfEt4WRBUOOfawQtCIM7H+S5cZlYL1APP4/P9DjdR7AE6gaecc37f53uBvwVCUcv8vL/gHbB/Z2a7zOy28LKk7PNifkC2zbJM13X6iJnlAz8H7nDOnTCb7U/uH865SaDOzAqBx8xsXYqLlDRm9nGg0zm3y8yuSHFx5tNlzrk2MysHnjKzV5P1QYu55t4C1ETNVwNtKSrLfOsws0qA8LgzxeVJODPLxAv2h51zvwgv9v1+AzjnjgNP451r8es+XwZca2aH8ZpUP2hmP8W/+wuAc64tPO4EHsNrXk7KPi/mcH8RWGNmq81sCdAEPJniMs2XJ4Et4ektwBMpLEvCmVdF/xfggHPunqiXfLvfZlYWrrFjZjnAh4BX8ek+O+e+7pyrds7V4v3f/Q/n3M34dH8BzCzPzAoi08BHgH0kaZ8X9R2qZvZRvHa7dOB+59y3UluixDOzR4Ar8LoF7QDuAh4HtgErgSPAjc652JOui5aZvQ/4A/Bnpttj78Rrd/flfpvZeryTael4la5tzrn/aWYl+HSfI8LNMl91zn3cz/trZufg1dbBaxL/mXPuW8na50Ud7iIiMrvF3CwjIiKnoXAXEfEhhbuIiA8p3EVEfEjhLiLiQwp3EREfUriLiPjQ/wcvl+7duoirdgAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(rmse[\"train\"])), rmse[\"train\"], color=\"blue\", label=\"train\")\n",
    "plt.plot(range(len(rmse[\"val\"])), rmse[\"val\"], color=\"red\", label=\"validation\")\n",
    "plt.title(\"RMSE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(best[\"state_dict\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# On test dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "test_pred = model.forward(torch.Tensor(test_timeseries), torch.Tensor(test_var_and_cor.to_numpy()), predict_anyway=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(index=test.index, columns=[\"id\", \"target\"])\n",
    "submission_df.id = test.id\n",
    "submission_df.target = test_pred.detach().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"RNN_submission.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28.082509,
   "end_time": "2021-06-28T09:53:45.178880",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-28T09:53:17.096371",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}