{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# demos for visualization\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "from nilearn.connectome import GroupSparseCovariance\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for ML\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# os library\n",
    "from os.path import exists, join"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "blessed-register",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-28T09:53:27.478513Z",
     "iopub.status.busy": "2021-06-28T09:53:27.474676Z",
     "iopub.status.idle": "2021-06-28T09:53:35.422989Z",
     "shell.execute_reply": "2021-06-28T09:53:35.421589Z",
     "shell.execute_reply.started": "2021-06-28T06:49:00.608859Z"
    },
    "papermill": {
     "duration": 7.970231,
     "end_time": "2021-06-28T09:53:35.423239",
     "exception": false,
     "start_time": "2021-06-28T09:53:27.453008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = join(\".\", \"data\") if exists(join(\".\", \"data\")) \\\n",
    "    else join(\"..\", \"input\", \"statistical-learning-sapienza-spring-2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train = pd.read_csv(join(data_path, 'train.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_data(entry):\n",
    "    id = entry.id\n",
    "    variables = entry[['var1', 'var2', 'var3']].to_numpy()\n",
    "    timeseries = entry[5 if \"y\" in entry.index else 4:].to_numpy(dtype=float).reshape((115, 116), order=\"F\")\n",
    "    if \"y\" in entry.index: return id, entry.y, variables, timeseries\n",
    "    else: return id, variables, timeseries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_split, val_split = train_test_split(train, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "def get_dataloader(timeseries, labels):\n",
    "    return DataLoader(TensorDataset(torch.Tensor(timeseries),torch.Tensor(labels)), batch_size=10) # create your dataloader\n",
    "\n",
    "train_timeseries = train_split.apply(lambda row: get_data(row)[-1], axis=1).to_list()\n",
    "train_dataloader = get_dataloader(train_timeseries, train_split.y.to_list())\n",
    "val_timeseries = val_split.apply(lambda row: get_data(row)[-1], axis=1).to_list()\n",
    "val_dataloader = get_dataloader(val_timeseries, val_split.y.to_list())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "class BlackEmbedding(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn_model = nn.RNN(input_size=116, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size*(1+num_layers), 32)\n",
    "        self.regressor = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x, predict_anyway=False):\n",
    "\n",
    "        x, h = self.rnn_model(x)\n",
    "\n",
    "        h = torch.cat(list(map(lambda i: h[i, ], range(h.shape[0]))), dim=-1)\n",
    "\n",
    "        x = torch.cat([x[:, -1, ], h], dim=-1)  # taking only the last\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        if self.training or predict_anyway:\n",
    "            x = self.regressor(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = BlackEmbedding(hidden_size=64, num_layers=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\SLHaka\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------> Epoch 1 <------\n",
      "    Train RMSE loss: 109.96170043945312\n",
      "    Val   RMSE loss: 108.21951293945312\n",
      "\n",
      "------> Epoch 2 <------\n",
      "    Train RMSE loss: 109.28860473632812\n",
      "    Val   RMSE loss: 107.01258087158203\n",
      "\n",
      "------> Epoch 3 <------\n",
      "    Train RMSE loss: 107.47926330566406\n",
      "    Val   RMSE loss: 104.6358642578125\n",
      "\n",
      "------> Epoch 4 <------\n",
      "    Train RMSE loss: 104.49647521972656\n",
      "    Val   RMSE loss: 101.17689514160156\n",
      "\n",
      "------> Epoch 5 <------\n",
      "    Train RMSE loss: 100.81016540527344\n",
      "    Val   RMSE loss: 97.32369995117188\n",
      "\n",
      "------> Epoch 6 <------\n",
      "    Train RMSE loss: 96.81134796142578\n",
      "    Val   RMSE loss: 93.18553161621094\n",
      "\n",
      "------> Epoch 7 <------\n",
      "    Train RMSE loss: 92.53305053710938\n",
      "    Val   RMSE loss: 88.68888092041016\n",
      "\n",
      "------> Epoch 8 <------\n",
      "    Train RMSE loss: 88.01128387451172\n",
      "    Val   RMSE loss: 84.05693817138672\n",
      "\n",
      "------> Epoch 9 <------\n",
      "    Train RMSE loss: 83.49514770507812\n",
      "    Val   RMSE loss: 79.33466339111328\n",
      "\n",
      "------> Epoch 10 <------\n",
      "    Train RMSE loss: 78.960693359375\n",
      "    Val   RMSE loss: 74.54351043701172\n",
      "\n",
      "------> Epoch 11 <------\n",
      "    Train RMSE loss: 74.24777221679688\n",
      "    Val   RMSE loss: 69.66488647460938\n",
      "\n",
      "------> Epoch 12 <------\n",
      "    Train RMSE loss: 69.90502166748047\n",
      "    Val   RMSE loss: 64.80760955810547\n",
      "\n",
      "------> Epoch 13 <------\n",
      "    Train RMSE loss: 65.41183471679688\n",
      "    Val   RMSE loss: 59.99867248535156\n",
      "\n",
      "------> Epoch 14 <------\n",
      "    Train RMSE loss: 60.562217712402344\n",
      "    Val   RMSE loss: 56.41108703613281\n",
      "\n",
      "------> Epoch 15 <------\n",
      "    Train RMSE loss: 55.69556427001953\n",
      "    Val   RMSE loss: 51.92945098876953\n",
      "\n",
      "------> Epoch 16 <------\n",
      "    Train RMSE loss: 51.34403991699219\n",
      "    Val   RMSE loss: 47.517311096191406\n",
      "\n",
      "------> Epoch 17 <------\n",
      "    Train RMSE loss: 46.9752082824707\n",
      "    Val   RMSE loss: 42.55128860473633\n",
      "\n",
      "------> Epoch 18 <------\n",
      "    Train RMSE loss: 43.2170524597168\n",
      "    Val   RMSE loss: 38.580909729003906\n",
      "\n",
      "------> Epoch 19 <------\n",
      "    Train RMSE loss: 39.2238655090332\n",
      "    Val   RMSE loss: 35.97377014160156\n",
      "\n",
      "------> Epoch 20 <------\n",
      "    Train RMSE loss: 35.96685028076172\n",
      "    Val   RMSE loss: 32.98958206176758\n",
      "\n",
      "------> Epoch 21 <------\n",
      "    Train RMSE loss: 32.91500473022461\n",
      "    Val   RMSE loss: 32.98593521118164\n",
      "\n",
      "------> Epoch 22 <------\n",
      "    Train RMSE loss: 29.901025772094727\n",
      "    Val   RMSE loss: 28.4338436126709\n",
      "\n",
      "------> Epoch 23 <------\n",
      "    Train RMSE loss: 28.230342864990234\n",
      "    Val   RMSE loss: 26.903947830200195\n",
      "\n",
      "------> Epoch 24 <------\n",
      "    Train RMSE loss: 26.74551010131836\n",
      "    Val   RMSE loss: 25.76280403137207\n",
      "\n",
      "------> Epoch 25 <------\n",
      "    Train RMSE loss: 25.661823272705078\n",
      "    Val   RMSE loss: 24.96101951599121\n",
      "\n",
      "------> Epoch 26 <------\n",
      "    Train RMSE loss: 24.609477996826172\n",
      "    Val   RMSE loss: 24.406156539916992\n",
      "\n",
      "------> Epoch 27 <------\n",
      "    Train RMSE loss: 24.696626663208008\n",
      "    Val   RMSE loss: 20.823558807373047\n",
      "\n",
      "------> Epoch 28 <------\n",
      "    Train RMSE loss: 24.155189514160156\n",
      "    Val   RMSE loss: 20.563587188720703\n",
      "\n",
      "------> Epoch 29 <------\n",
      "    Train RMSE loss: 23.93834686279297\n",
      "    Val   RMSE loss: 20.386241912841797\n",
      "\n",
      "------> Epoch 30 <------\n",
      "    Train RMSE loss: 24.129365921020508\n",
      "    Val   RMSE loss: 20.268571853637695\n",
      "\n",
      "\n",
      "****************** Finished Training ******************\n",
      "seconds elapsed:  145.84605264663696\n"
     ]
    }
   ],
   "source": [
    "epocs = 30\n",
    "rmse = {\"train\": [], \"val\": []}\n",
    "\n",
    "start = time()\n",
    "for epoch in range(epocs):\n",
    "    model.train()\n",
    "    rmse[\"train\"].append(0)\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        rmse[\"train\"][epoch] += loss.sqrt().mean(dim=0)/len(train_dataloader)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "\n",
    "    rmse[\"val\"].append(0)\n",
    "    for i, data in enumerate(val_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, predict_anyway=True)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        rmse[\"val\"][epoch] += criterion(outputs, labels).sqrt().mean(dim=0)/len(val_dataloader)\n",
    "    print(f\"\\n------> Epoch {epoch+1} <------\\n    Train RMSE loss: {rmse['train'][epoch]}\\n    Val   RMSE loss: {rmse['val'][epoch]}\")\n",
    "\n",
    "print('\\n\\n****************** Finished Training ******************')\n",
    "print(\"seconds elapsed: \", time()-start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(rmse[\"train\"])), rmse[\"train\"], color=\"blue\", label=\"train\")\n",
    "plt.plot(range(len(rmse[\"val\"])), rmse[\"val\"], color=\"red\", label=\"validation\")\n",
    "plt.title(\"RMSE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "train_embedding = model.forward(torch.Tensor(train_timeseries))\n",
    "val_embedding = model.forward(torch.Tensor(val_timeseries))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "train_embedding_df = pd.concat([train_split.iloc[:, 2:4], pd.DataFrame(train_embedding.detach().numpy(), index=train_split.index)], axis=1)\n",
    "val_embedding_df = pd.concat([val_split.iloc[:, 2:4], pd.DataFrame(val_embedding.detach().numpy(), index=val_split.index)], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "train_embedding_df.var2 = train_embedding_df.var2.replace({\"A\": 0, \"C\": 1})\n",
    "val_embedding_df.var2 = val_embedding_df.var2.replace({\"A\": 0, \"C\": 1})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "X_train, X_val = train_embedding_df.to_numpy(), val_embedding_df.to_numpy()\n",
    "y_train, y_val = train_split.y.to_numpy(), val_split.y.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [
    {
     "data": {
      "text/plain": "SVR(C=0.5, degree=4, kernel='sigmoid')"
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = svm.SVR(shrinking=True, kernel=\"sigmoid\", gamma=\"scale\", degree=4, C=0.5)\n",
    "\n",
    "svm_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "y_ped_train = svm_model.predict(X_train)\n",
    "y_ped_val = svm_model.predict(X_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 14.46731332919218 \n",
      "val  : 16.120551887566872\n"
     ]
    }
   ],
   "source": [
    "print(f\"train : {np.sqrt(mean_squared_error(y_ped_train, y_train))} \\nval  : {np.sqrt(mean_squared_error(y_ped_val, y_val))}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "data": {
      "text/plain": "           0    1          2         3          4    5          6          7   \\\n110  0.000000  0.0  33.637390  0.000000  33.896420  0.0  30.615675  31.881088   \n419  0.000000  0.0  33.666306  0.000000  33.922497  0.0  30.645451  31.905592   \n565  0.000000  0.0  33.663048  0.000000  33.919193  0.0  30.641996  31.902082   \n77   0.000000  0.0  25.453283  0.000000  26.474634  0.0  23.194553  24.252129   \n181  0.000000  0.0  33.622978  0.000000  33.884037  0.0  30.611977  31.857613   \n..        ...  ...        ...       ...        ...  ...        ...        ...   \n399  0.000000  0.0  33.666332  0.000000  33.922523  0.0  30.645472  31.905622   \n340  0.000000  0.0  33.666298  0.000000  33.922527  0.0  30.645456  31.905621   \n148  0.042667  0.0  32.572865  0.100658  32.659508  0.0  29.666977  30.599995   \n494  0.000000  0.0  33.655487  0.000000  33.915691  0.0  30.638044  31.897831   \n439  0.000000  0.0  33.666225  0.000000  33.922428  0.0  30.645372  31.905540   \n\n            8    9   ...         22      23         24   25         26  \\\n110  31.626308  0.0  ...  28.560516  0.0000  33.136833  0.0  30.388554   \n419  31.652485  0.0  ...  28.585737  0.0000  33.165642  0.0  30.417480   \n565  31.648861  0.0  ...  28.582979  0.0000  33.162563  0.0  30.414177   \n77   24.933500  0.0  ...  21.665619  0.2713  25.743305  0.0  22.891548   \n181  31.602100  0.0  ...  28.555353  0.0000  33.120586  0.0  30.365826   \n..         ...  ...  ...        ...     ...        ...  ...        ...   \n399  31.652496  0.0  ...  28.585690  0.0000  33.165638  0.0  30.417484   \n340  31.652515  0.0  ...  28.585758  0.0000  33.165642  0.0  30.417496   \n148  30.702549  0.0  ...  27.752827  0.0000  32.023682  0.0  29.502060   \n494  31.643623  0.0  ...  28.577787  0.0000  33.156681  0.0  30.407158   \n439  31.652405  0.0  ...  28.585634  0.0000  33.165539  0.0  30.417383   \n\n            27         28         29   30        31  \n110  31.317009  30.641258  29.553339  0.0  0.000000  \n419  31.347656  30.668304  29.575769  0.0  0.000000  \n565  31.344082  30.664511  29.572756  0.0  0.000000  \n77   24.141226  23.771708  22.632118  0.0  0.032363  \n181  31.302959  30.623489  29.535894  0.0  0.000000  \n..         ...        ...        ...  ...       ...  \n399  31.347687  30.668293  29.575781  0.0  0.000000  \n340  31.347713  30.668343  29.575809  0.0  0.000000  \n148  30.236879  29.531418  28.562664  0.0  0.000000  \n494  31.340368  30.659786  29.570171  0.0  0.000000  \n439  31.347620  30.668217  29.575720  0.0  0.000000  \n\n[120 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>110</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>33.637390</td>\n      <td>0.000000</td>\n      <td>33.896420</td>\n      <td>0.0</td>\n      <td>30.615675</td>\n      <td>31.881088</td>\n      <td>31.626308</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>28.560516</td>\n      <td>0.0000</td>\n      <td>33.136833</td>\n      <td>0.0</td>\n      <td>30.388554</td>\n      <td>31.317009</td>\n      <td>30.641258</td>\n      <td>29.553339</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>419</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>33.666306</td>\n      <td>0.000000</td>\n      <td>33.922497</td>\n      <td>0.0</td>\n      <td>30.645451</td>\n      <td>31.905592</td>\n      <td>31.652485</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>28.585737</td>\n      <td>0.0000</td>\n      <td>33.165642</td>\n      <td>0.0</td>\n      <td>30.417480</td>\n      <td>31.347656</td>\n      <td>30.668304</td>\n      <td>29.575769</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>33.663048</td>\n      <td>0.000000</td>\n      <td>33.919193</td>\n      <td>0.0</td>\n      <td>30.641996</td>\n      <td>31.902082</td>\n      <td>31.648861</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>28.582979</td>\n      <td>0.0000</td>\n      <td>33.162563</td>\n      <td>0.0</td>\n      <td>30.414177</td>\n      <td>31.344082</td>\n      <td>30.664511</td>\n      <td>29.572756</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>25.453283</td>\n      <td>0.000000</td>\n      <td>26.474634</td>\n      <td>0.0</td>\n      <td>23.194553</td>\n      <td>24.252129</td>\n      <td>24.933500</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>21.665619</td>\n      <td>0.2713</td>\n      <td>25.743305</td>\n      <td>0.0</td>\n      <td>22.891548</td>\n      <td>24.141226</td>\n      <td>23.771708</td>\n      <td>22.632118</td>\n      <td>0.0</td>\n      <td>0.032363</td>\n    </tr>\n    <tr>\n      <th>181</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>33.622978</td>\n      <td>0.000000</td>\n      <td>33.884037</td>\n      <td>0.0</td>\n      <td>30.611977</td>\n      <td>31.857613</td>\n      <td>31.602100</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>28.555353</td>\n      <td>0.0000</td>\n      <td>33.120586</td>\n      <td>0.0</td>\n      <td>30.365826</td>\n      <td>31.302959</td>\n      <td>30.623489</td>\n      <td>29.535894</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>33.666332</td>\n      <td>0.000000</td>\n      <td>33.922523</td>\n      <td>0.0</td>\n      <td>30.645472</td>\n      <td>31.905622</td>\n      <td>31.652496</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>28.585690</td>\n      <td>0.0000</td>\n      <td>33.165638</td>\n      <td>0.0</td>\n      <td>30.417484</td>\n      <td>31.347687</td>\n      <td>30.668293</td>\n      <td>29.575781</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>340</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>33.666298</td>\n      <td>0.000000</td>\n      <td>33.922527</td>\n      <td>0.0</td>\n      <td>30.645456</td>\n      <td>31.905621</td>\n      <td>31.652515</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>28.585758</td>\n      <td>0.0000</td>\n      <td>33.165642</td>\n      <td>0.0</td>\n      <td>30.417496</td>\n      <td>31.347713</td>\n      <td>30.668343</td>\n      <td>29.575809</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>0.042667</td>\n      <td>0.0</td>\n      <td>32.572865</td>\n      <td>0.100658</td>\n      <td>32.659508</td>\n      <td>0.0</td>\n      <td>29.666977</td>\n      <td>30.599995</td>\n      <td>30.702549</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>27.752827</td>\n      <td>0.0000</td>\n      <td>32.023682</td>\n      <td>0.0</td>\n      <td>29.502060</td>\n      <td>30.236879</td>\n      <td>29.531418</td>\n      <td>28.562664</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>494</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>33.655487</td>\n      <td>0.000000</td>\n      <td>33.915691</td>\n      <td>0.0</td>\n      <td>30.638044</td>\n      <td>31.897831</td>\n      <td>31.643623</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>28.577787</td>\n      <td>0.0000</td>\n      <td>33.156681</td>\n      <td>0.0</td>\n      <td>30.407158</td>\n      <td>31.340368</td>\n      <td>30.659786</td>\n      <td>29.570171</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>439</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>33.666225</td>\n      <td>0.000000</td>\n      <td>33.922428</td>\n      <td>0.0</td>\n      <td>30.645372</td>\n      <td>31.905540</td>\n      <td>31.652405</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>28.585634</td>\n      <td>0.0000</td>\n      <td>33.165539</td>\n      <td>0.0</td>\n      <td>30.417383</td>\n      <td>31.347620</td>\n      <td>30.668217</td>\n      <td>29.575720</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>120 rows × 32 columns</p>\n</div>"
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(val_embedding.detach().numpy(), index=val_split.index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "test = pd.read_csv(join(data_path, 'test.csv'))\n",
    "time_series_test = test.apply(lambda row: get_data(row)[-1], axis=1).to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [],
   "source": [
    "predictions = model.forward(torch.Tensor(time_series_test), predict_anyway=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "test_embedding_df = pd.concat([test.iloc[:, 2:4], pd.DataFrame(test_embedding.detach().numpy(), index=test.index)], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[106.7174],\n        [106.8461],\n        [106.8462],\n        [ 40.6383],\n        [106.8414],\n        [106.8447],\n        [106.8461],\n        [ 97.3616],\n        [ 35.5079],\n        [106.8461],\n        [ 93.9863],\n        [106.8318],\n        [-39.6860],\n        [106.8460],\n        [106.8461],\n        [106.8429],\n        [106.8459],\n        [106.8445],\n        [106.8419],\n        [-19.4348],\n        [106.8454],\n        [106.8446],\n        [106.8400],\n        [106.8461],\n        [106.8461],\n        [106.8448],\n        [-38.0568],\n        [106.6906],\n        [106.8460],\n        [106.8461],\n        [106.8459],\n        [ 97.5422],\n        [ 21.0665],\n        [106.8434],\n        [106.8444],\n        [106.8461],\n        [101.3131],\n        [106.8457],\n        [106.8461],\n        [106.8458],\n        [106.8461],\n        [106.8453],\n        [106.8457],\n        [106.8460],\n        [106.8461],\n        [106.8441],\n        [ 14.8656],\n        [106.3747],\n        [ 20.8572],\n        [106.8396],\n        [106.8461],\n        [106.8460],\n        [106.8441],\n        [106.8443],\n        [106.8458],\n        [106.8461],\n        [106.8451],\n        [ 98.0887],\n        [ 63.8200],\n        [106.8455],\n        [ 61.1476],\n        [106.8453],\n        [106.8461],\n        [106.8459],\n        [106.8459],\n        [ 27.2343],\n        [106.8430],\n        [106.8449],\n        [-31.3093],\n        [106.8461],\n        [106.8457],\n        [106.8461],\n        [106.8461],\n        [106.8461],\n        [106.8461],\n        [106.8417],\n        [106.8452],\n        [106.8461],\n        [106.8460],\n        [106.8461],\n        [106.8458],\n        [106.8461],\n        [106.8460],\n        [106.8461],\n        [  7.5869],\n        [106.8460],\n        [ 91.2791],\n        [106.8087],\n        [106.8461],\n        [106.8461],\n        [106.8460],\n        [106.8460],\n        [ 58.7769],\n        [106.8462],\n        [106.8455],\n        [106.8460],\n        [106.8461],\n        [106.8461],\n        [106.8461],\n        [ 33.7541],\n        [ -4.9137],\n        [106.8460],\n        [106.8461],\n        [106.8461],\n        [ 54.4752],\n        [106.8461],\n        [ 87.6812],\n        [106.8459],\n        [106.8461],\n        [106.8461],\n        [106.8459],\n        [ 47.6056],\n        [106.8449],\n        [106.8459],\n        [106.8461],\n        [106.8461],\n        [106.8461],\n        [105.0062],\n        [106.8458],\n        [106.8461]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "y_ped_val = model.forward(torch.Tensor(val_timeseries), predict_anyway=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28.082509,
   "end_time": "2021-06-28T09:53:45.178880",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-28T09:53:17.096371",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}